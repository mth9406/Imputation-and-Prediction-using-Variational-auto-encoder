{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, data_path='./data/spam', data_type='spam', delta=0.0, drop_p=0.5, epoch=1, imp_loss_penalty=1.0, input_size=20, kl_weight=0.1, lr=0.0003, model_file='latest_checkpoint.pth.tar', model_path='data/mobile/vai', model_type='vibi', n_labels=4, num_folds=1, patience=10, print_log_option=1, prob=0.1, stack_fc_lyrs=False, standardize=True, test=False, test_all_missing=True, test_n_missing=2, tr=0.7, vai_n_samples=100, val=0.2)\n",
      "The path already exists, skip making the path...\n",
      "Loading data...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4601 non-null   float64\n",
      " 1   1       4601 non-null   float64\n",
      " 2   2       4601 non-null   float64\n",
      " 3   3       4601 non-null   float64\n",
      " 4   4       4601 non-null   float64\n",
      " 5   5       4601 non-null   float64\n",
      " 6   6       4601 non-null   float64\n",
      " 7   7       4601 non-null   float64\n",
      " 8   8       4601 non-null   float64\n",
      " 9   9       4601 non-null   float64\n",
      " 10  10      4601 non-null   float64\n",
      " 11  11      4601 non-null   float64\n",
      " 12  12      4601 non-null   float64\n",
      " 13  13      4601 non-null   float64\n",
      " 14  14      4601 non-null   float64\n",
      " 15  15      4601 non-null   float64\n",
      " 16  16      4601 non-null   float64\n",
      " 17  17      4601 non-null   float64\n",
      " 18  18      4601 non-null   float64\n",
      " 19  19      4601 non-null   float64\n",
      " 20  20      4601 non-null   float64\n",
      " 21  21      4601 non-null   float64\n",
      " 22  22      4601 non-null   float64\n",
      " 23  23      4601 non-null   float64\n",
      " 24  24      4601 non-null   float64\n",
      " 25  25      4601 non-null   float64\n",
      " 26  26      4601 non-null   float64\n",
      " 27  27      4601 non-null   float64\n",
      " 28  28      4601 non-null   float64\n",
      " 29  29      4601 non-null   float64\n",
      " 30  30      4601 non-null   float64\n",
      " 31  31      4601 non-null   float64\n",
      " 32  32      4601 non-null   float64\n",
      " 33  33      4601 non-null   float64\n",
      " 34  34      4601 non-null   float64\n",
      " 35  35      4601 non-null   float64\n",
      " 36  36      4601 non-null   float64\n",
      " 37  37      4601 non-null   float64\n",
      " 38  38      4601 non-null   float64\n",
      " 39  39      4601 non-null   float64\n",
      " 40  40      4601 non-null   float64\n",
      " 41  41      4601 non-null   float64\n",
      " 42  42      4601 non-null   float64\n",
      " 43  43      4601 non-null   float64\n",
      " 44  44      4601 non-null   float64\n",
      " 45  45      4601 non-null   float64\n",
      " 46  46      4601 non-null   float64\n",
      " 47  47      4601 non-null   float64\n",
      " 48  48      4601 non-null   float64\n",
      " 49  49      4601 non-null   float64\n",
      " 50  50      4601 non-null   float64\n",
      " 51  51      4601 non-null   float64\n",
      " 52  52      4601 non-null   float64\n",
      " 53  53      4601 non-null   float64\n",
      " 54  54      4601 non-null   float64\n",
      " 55  55      4601 non-null   int64  \n",
      " 56  56      4601 non-null   int64  \n",
      " 57  57      4601 non-null   int64  \n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.1 MB\n",
      "None\n",
      "--------------------\n",
      "every observation has 2 missing values out of 57...\n",
      "Loading data done!\n",
      "Start training...\n",
      "Epoch [1/1] Batch [1/26]:                     loss = 6.377021312713623\n",
      "Epoch [1/1] Batch [2/26]:                     loss = 7.774588584899902\n",
      "Epoch [1/1] Batch [3/26]:                     loss = 7.857081413269043\n",
      "Epoch [1/1] Batch [4/26]:                     loss = 6.211568355560303\n",
      "Epoch [1/1] Batch [5/26]:                     loss = 6.116451740264893\n",
      "Epoch [1/1] Batch [6/26]:                     loss = 5.024896144866943\n",
      "Epoch [1/1] Batch [7/26]:                     loss = 6.002731800079346\n",
      "Epoch [1/1] Batch [8/26]:                     loss = 6.1162943840026855\n",
      "Epoch [1/1] Batch [9/26]:                     loss = 8.820881843566895\n",
      "Epoch [1/1] Batch [10/26]:                     loss = 5.531250476837158\n",
      "Epoch [1/1] Batch [11/26]:                     loss = 6.156425476074219\n",
      "Epoch [1/1] Batch [12/26]:                     loss = 6.978410243988037\n",
      "Epoch [1/1] Batch [13/26]:                     loss = 5.154275417327881\n",
      "Epoch [1/1] Batch [14/26]:                     loss = 5.484241485595703\n",
      "Epoch [1/1] Batch [15/26]:                     loss = 9.201606750488281\n",
      "Epoch [1/1] Batch [16/26]:                     loss = 6.130168914794922\n",
      "Epoch [1/1] Batch [17/26]:                     loss = 6.3905792236328125\n",
      "Epoch [1/1] Batch [18/26]:                     loss = 8.243680000305176\n",
      "Epoch [1/1] Batch [19/26]:                     loss = 6.2404465675354\n",
      "Epoch [1/1] Batch [20/26]:                     loss = 6.100976467132568\n",
      "Epoch [1/1] Batch [21/26]:                     loss = 7.87169885635376\n",
      "Epoch [1/1] Batch [22/26]:                     loss = 12.760079383850098\n",
      "Epoch [1/1] Batch [23/26]:                     loss = 5.17664909362793\n",
      "Epoch [1/1] Batch [24/26]:                     loss = 5.23219633102417\n",
      "Epoch [1/1] Batch [25/26]:                     loss = 11.163078308105469\n",
      "Epoch [1/1] Batch [26/26]:                     loss = 4.377267837524414\n",
      "Epoch [1/1]: training loss= 6.865175, validation loss= 0.705929\n",
      "Validation loss decreased: inf --> 0.7059. Saving model...\n",
      "Training done! Saving logs...\n",
      "Testing the model...\n",
      "Test done!\n",
      "test total loss: 0.66\n",
      "test imputation loss: 0.00\n",
      "test prediction loss: 0.66\n",
      "test imputation prediction loss 0.71\n",
      "\n",
      "Figure(640x480)\n",
      "정확도 (accuracy): 0.62\n",
      "재현율 (recall): 0.61\n",
      "정밀도 (precision): 0.61\n",
      "F1 score: 0.61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['NanumGothic'] not found. Falling back to DejaVu Sans.\n"
     ]
    }
   ],
   "source": [
    "!python main/main.py --data_type spam --data_path ./data/spam --standardize --batch_size 128 --epoch 1 --lr 3e-4 --patience 10 --print_log_option 1 --model_path data/mobile/vai --input_size 20 --n_labels 4 --model_type vibi --num_folds 1 --test_all_missing --test_n_missing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, data_path='./data/breast', data_type='breast', delta=0.0, drop_p=0.5, epoch=1, imp_loss_penalty=1.0, input_size=18, kl_weight=0.1, lr=0.0003, model_file='latest_checkpoint.pth.tar', model_path='data/mobile/vai', model_type='vibi', n_labels=5, num_folds=1, patience=10, print_log_option=1, prob=0.1, stack_fc_lyrs=False, standardize=True, test=False, test_all_missing=True, test_n_missing=2, tr=0.7, vai_n_samples=100, val=0.2)\n",
      "The path already exists, skip making the path...\n",
      "Loading data...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 683 entries, 0 to 698\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       683 non-null    int64\n",
      " 1   1       683 non-null    int64\n",
      " 2   2       683 non-null    int64\n",
      " 3   3       683 non-null    int64\n",
      " 4   4       683 non-null    int64\n",
      " 5   5       683 non-null    int64\n",
      " 6   6       683 non-null    int32\n",
      " 7   7       683 non-null    int64\n",
      " 8   8       683 non-null    int64\n",
      " 9   9       683 non-null    int64\n",
      " 10  10      683 non-null    int64\n",
      "dtypes: int32(1), int64(10)\n",
      "memory usage: 61.4 KB\n",
      "None\n",
      "--------------------\n",
      "every observation has 2 missing values out of 10...\n",
      "Loading data done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"main/main.py\", line 218, in <module>\n",
      "    main(args)\n",
      "  File \"main/main.py\", line 189, in main\n",
      "    w = compute_class_weight(class_weight='balanced', classes= np.arange(args.n_labels), y= y_train.numpy())\n",
      "  File \"c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 41, in compute_class_weight\n",
      "    raise ValueError(\"classes should include all valid labels that can be in y\")\n",
      "ValueError: classes should include all valid labels that can be in y\n"
     ]
    }
   ],
   "source": [
    "!python main/main.py --data_type breast --data_path ./data/breast --standardize --batch_size 128 --epoch 1 --lr 3e-4 --patience 10 --print_log_option 1 --model_path data/mobile/vai --model_type vibi --num_folds 1 --test_all_missing --test_n_missing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, data_path='./data/letter', data_type='letter', delta=0.0, drop_p=0.5, epoch=1, imp_loss_penalty=1.0, input_size=18, kl_weight=0.1, lr=0.0003, model_file='latest_checkpoint.pth.tar', model_path='data/mobile/vai', model_type='vibi', n_labels=5, num_folds=1, patience=10, print_log_option=1, prob=0.1, stack_fc_lyrs=False, standardize=True, test=False, test_all_missing=True, test_n_missing=2, tr=0.7, vai_n_samples=100, val=0.2)\n",
      "The path already exists, skip making the path...\n",
      "Loading data...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       20000 non-null  object\n",
      " 1   1       20000 non-null  int64 \n",
      " 2   2       20000 non-null  int64 \n",
      " 3   3       20000 non-null  int64 \n",
      " 4   4       20000 non-null  int64 \n",
      " 5   5       20000 non-null  int64 \n",
      " 6   6       20000 non-null  int64 \n",
      " 7   7       20000 non-null  int64 \n",
      " 8   8       20000 non-null  int64 \n",
      " 9   9       20000 non-null  int64 \n",
      " 10  10      20000 non-null  int64 \n",
      " 11  11      20000 non-null  int64 \n",
      " 12  12      20000 non-null  int64 \n",
      " 13  13      20000 non-null  int64 \n",
      " 14  14      20000 non-null  int64 \n",
      " 15  15      20000 non-null  int64 \n",
      " 16  16      20000 non-null  int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 2.7+ MB\n",
      "None\n",
      "--------------------\n",
      "every observation has 2 missing values out of 16...\n",
      "Loading data done!\n",
      "Start training...\n",
      "Epoch [1/1] Batch [1/110]:                     loss = 7.978088855743408\n",
      "Epoch [1/1] Batch [2/110]:                     loss = 8.568182945251465\n",
      "Epoch [1/1] Batch [3/110]:                     loss = 7.929116249084473\n",
      "Epoch [1/1] Batch [4/110]:                     loss = 8.480690002441406\n",
      "Epoch [1/1] Batch [5/110]:                     loss = 8.316976547241211\n",
      "Epoch [1/1] Batch [6/110]:                     loss = 7.876997947692871\n",
      "Epoch [1/1] Batch [7/110]:                     loss = 8.564009666442871\n",
      "Epoch [1/1] Batch [8/110]:                     loss = 8.352141380310059\n",
      "Epoch [1/1] Batch [9/110]:                     loss = 8.326244354248047\n",
      "Epoch [1/1] Batch [10/110]:                     loss = 8.32375717163086\n",
      "Epoch [1/1] Batch [11/110]:                     loss = 8.2594575881958\n",
      "Epoch [1/1] Batch [12/110]:                     loss = 8.579195976257324\n",
      "Epoch [1/1] Batch [13/110]:                     loss = 8.510638236999512\n",
      "Epoch [1/1] Batch [14/110]:                     loss = 8.292202949523926\n",
      "Epoch [1/1] Batch [15/110]:                     loss = 8.258238792419434\n",
      "Epoch [1/1] Batch [16/110]:                     loss = 8.059174537658691\n",
      "Epoch [1/1] Batch [17/110]:                     loss = 8.25968074798584\n",
      "Epoch [1/1] Batch [18/110]:                     loss = 8.53014850616455\n",
      "Epoch [1/1] Batch [19/110]:                     loss = 8.179819107055664\n",
      "Epoch [1/1] Batch [20/110]:                     loss = 8.448498725891113\n",
      "Epoch [1/1] Batch [21/110]:                     loss = 8.042734146118164\n",
      "Epoch [1/1] Batch [22/110]:                     loss = 8.108609199523926\n",
      "Epoch [1/1] Batch [23/110]:                     loss = 8.00124740600586\n",
      "Epoch [1/1] Batch [24/110]:                     loss = 8.034876823425293\n",
      "Epoch [1/1] Batch [25/110]:                     loss = 8.191564559936523\n",
      "Epoch [1/1] Batch [26/110]:                     loss = 8.326269149780273\n",
      "Epoch [1/1] Batch [27/110]:                     loss = 8.919550895690918\n",
      "Epoch [1/1] Batch [28/110]:                     loss = 7.891831874847412\n",
      "Epoch [1/1] Batch [29/110]:                     loss = 8.418200492858887\n",
      "Epoch [1/1] Batch [30/110]:                     loss = 8.074156761169434\n",
      "Epoch [1/1] Batch [31/110]:                     loss = 8.072829246520996\n",
      "Epoch [1/1] Batch [32/110]:                     loss = 8.169265747070312\n",
      "Epoch [1/1] Batch [33/110]:                     loss = 8.52000617980957\n",
      "Epoch [1/1] Batch [34/110]:                     loss = 7.736573696136475\n",
      "Epoch [1/1] Batch [35/110]:                     loss = 7.978701114654541\n",
      "Epoch [1/1] Batch [36/110]:                     loss = 8.15295696258545\n",
      "Epoch [1/1] Batch [37/110]:                     loss = 8.124340057373047\n",
      "Epoch [1/1] Batch [38/110]:                     loss = 7.968534469604492\n",
      "Epoch [1/1] Batch [39/110]:                     loss = 8.213815689086914\n",
      "Epoch [1/1] Batch [40/110]:                     loss = 8.276142120361328\n",
      "Epoch [1/1] Batch [41/110]:                     loss = 8.266519546508789\n",
      "Epoch [1/1] Batch [42/110]:                     loss = 7.747641563415527\n",
      "Epoch [1/1] Batch [43/110]:                     loss = 8.346040725708008\n",
      "Epoch [1/1] Batch [44/110]:                     loss = 7.9187541007995605\n",
      "Epoch [1/1] Batch [45/110]:                     loss = 7.809539794921875\n",
      "Epoch [1/1] Batch [46/110]:                     loss = 8.040179252624512\n",
      "Epoch [1/1] Batch [47/110]:                     loss = 8.164651870727539\n",
      "Epoch [1/1] Batch [48/110]:                     loss = 8.503633499145508\n",
      "Epoch [1/1] Batch [49/110]:                     loss = 7.731086254119873\n",
      "Epoch [1/1] Batch [50/110]:                     loss = 7.866854190826416\n",
      "Epoch [1/1] Batch [51/110]:                     loss = 7.883718013763428\n",
      "Epoch [1/1] Batch [52/110]:                     loss = 8.008177757263184\n",
      "Epoch [1/1] Batch [53/110]:                     loss = 8.071533203125\n",
      "Epoch [1/1] Batch [54/110]:                     loss = 7.862125873565674\n",
      "Epoch [1/1] Batch [55/110]:                     loss = 7.739658832550049\n",
      "Epoch [1/1] Batch [56/110]:                     loss = 7.9186272621154785\n",
      "Epoch [1/1] Batch [57/110]:                     loss = 7.906351089477539\n",
      "Epoch [1/1] Batch [58/110]:                     loss = 8.226046562194824\n",
      "Epoch [1/1] Batch [59/110]:                     loss = 7.554651260375977\n",
      "Epoch [1/1] Batch [60/110]:                     loss = 7.98061466217041\n",
      "Epoch [1/1] Batch [61/110]:                     loss = 7.688525199890137\n",
      "Epoch [1/1] Batch [62/110]:                     loss = 7.766854763031006\n",
      "Epoch [1/1] Batch [63/110]:                     loss = 7.886127471923828\n",
      "Epoch [1/1] Batch [64/110]:                     loss = 8.088353157043457\n",
      "Epoch [1/1] Batch [65/110]:                     loss = 7.662694454193115\n",
      "Epoch [1/1] Batch [66/110]:                     loss = 8.137514114379883\n",
      "Epoch [1/1] Batch [67/110]:                     loss = 7.703564167022705\n",
      "Epoch [1/1] Batch [68/110]:                     loss = 7.9211015701293945\n",
      "Epoch [1/1] Batch [69/110]:                     loss = 7.591712951660156\n",
      "Epoch [1/1] Batch [70/110]:                     loss = 7.790840148925781\n",
      "Epoch [1/1] Batch [71/110]:                     loss = 7.451818466186523\n",
      "Epoch [1/1] Batch [72/110]:                     loss = 7.7861647605896\n",
      "Epoch [1/1] Batch [73/110]:                     loss = 7.960540294647217\n",
      "Epoch [1/1] Batch [74/110]:                     loss = 7.7350544929504395\n",
      "Epoch [1/1] Batch [75/110]:                     loss = 7.726291179656982\n",
      "Epoch [1/1] Batch [76/110]:                     loss = 7.4962615966796875\n",
      "Epoch [1/1] Batch [77/110]:                     loss = 7.612673282623291\n",
      "Epoch [1/1] Batch [78/110]:                     loss = 8.103883743286133\n",
      "Epoch [1/1] Batch [79/110]:                     loss = 7.627463340759277\n",
      "Epoch [1/1] Batch [80/110]:                     loss = 7.842411994934082\n",
      "Epoch [1/1] Batch [81/110]:                     loss = 7.782900333404541\n",
      "Epoch [1/1] Batch [82/110]:                     loss = 8.085714340209961\n",
      "Epoch [1/1] Batch [83/110]:                     loss = 7.568338871002197\n",
      "Epoch [1/1] Batch [84/110]:                     loss = 7.874080657958984\n",
      "Epoch [1/1] Batch [85/110]:                     loss = 7.588504314422607\n",
      "Epoch [1/1] Batch [86/110]:                     loss = 7.50896692276001\n",
      "Epoch [1/1] Batch [87/110]:                     loss = 7.571316242218018\n",
      "Epoch [1/1] Batch [88/110]:                     loss = 7.656804084777832\n",
      "Epoch [1/1] Batch [89/110]:                     loss = 7.614332675933838\n",
      "Epoch [1/1] Batch [90/110]:                     loss = 7.877020835876465\n",
      "Epoch [1/1] Batch [91/110]:                     loss = 7.570437908172607\n",
      "Epoch [1/1] Batch [92/110]:                     loss = 7.433941841125488\n",
      "Epoch [1/1] Batch [93/110]:                     loss = 7.603997230529785\n",
      "Epoch [1/1] Batch [94/110]:                     loss = 7.7069830894470215\n",
      "Epoch [1/1] Batch [95/110]:                     loss = 7.5392537117004395\n",
      "Epoch [1/1] Batch [96/110]:                     loss = 7.5050249099731445\n",
      "Epoch [1/1] Batch [97/110]:                     loss = 7.687244892120361\n",
      "Epoch [1/1] Batch [98/110]:                     loss = 7.578158378601074\n",
      "Epoch [1/1] Batch [99/110]:                     loss = 7.52146053314209\n",
      "Epoch [1/1] Batch [100/110]:                     loss = 7.551789283752441\n",
      "Epoch [1/1] Batch [101/110]:                     loss = 7.516262054443359\n",
      "Epoch [1/1] Batch [102/110]:                     loss = 7.718419551849365\n",
      "Epoch [1/1] Batch [103/110]:                     loss = 7.64026403427124\n",
      "Epoch [1/1] Batch [104/110]:                     loss = 7.450754642486572\n",
      "Epoch [1/1] Batch [105/110]:                     loss = 7.340239524841309\n",
      "Epoch [1/1] Batch [106/110]:                     loss = 7.794173717498779\n",
      "Epoch [1/1] Batch [107/110]:                     loss = 7.658819198608398\n",
      "Epoch [1/1] Batch [108/110]:                     loss = 7.596473693847656\n",
      "Epoch [1/1] Batch [109/110]:                     loss = 7.3297882080078125\n",
      "Epoch [1/1] Batch [110/110]:                     loss = 7.757143020629883\n",
      "Epoch [1/1]: training loss= 7.937003, validation loss= 3.473002\n",
      "Validation loss decreased: inf --> 3.4730. Saving model...\n",
      "Training done! Saving logs...\n",
      "Testing the model...\n",
      "Test done!\n",
      "test total loss: 3.37\n",
      "test imputation loss: 0.00\n",
      "test prediction loss: 3.37\n",
      "test imputation prediction loss 1.02\n",
      "\n",
      "Figure(640x480)\n",
      "정확도 (accuracy): 0.04\n",
      "재현율 (recall): 0.04\n",
      "정밀도 (precision): 0.05\n",
      "F1 score: 0.04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['NanumGothic'] not found. Falling back to DejaVu Sans.\n"
     ]
    }
   ],
   "source": [
    "!python main/main.py --data_type letter --data_path ./data/letter --standardize --batch_size 128 --epoch 1 --lr 3e-4 --patience 10 --print_log_option 1 --model_path data/mobile/vai --model_type vibi --num_folds 1 --test_all_missing --test_n_missing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/breast/breast-cancer-wisconsin.data', header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   10\n",
       "0  1000025   5   1   1   1   2   1   3   1   1   2\n",
       "1  1002945   5   4   4   5   7  10   3   2   1   2\n",
       "2  1015425   3   1   1   1   2   2   3   1   1   2\n",
       "3  1016277   6   8   8   1   3   4   3   7   1   2\n",
       "4  1017023   4   1   1   3   2   1   3   1   1   2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 699 entries, 0 to 698\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       699 non-null    int64 \n",
      " 1   1       699 non-null    int64 \n",
      " 2   2       699 non-null    int64 \n",
      " 3   3       699 non-null    int64 \n",
      " 4   4       699 non-null    int64 \n",
      " 5   5       699 non-null    int64 \n",
      " 6   6       699 non-null    object\n",
      " 7   7       699 non-null    int64 \n",
      " 8   8       699 non-null    int64 \n",
      " 9   9       699 non-null    int64 \n",
      " 10  10      699 non-null    int64 \n",
      "dtypes: int64(10), object(1)\n",
      "memory usage: 60.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.iloc[:, 6] == '?'\n",
    "data =  data.loc[~idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 6] = data.iloc[:, 6].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
